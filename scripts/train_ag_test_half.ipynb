{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26726\n",
      "26726\n",
      "(26726, 100)\n",
      "(25159, 5)\n",
      "(564, 17775)\n",
      "(564, 26251)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "importlib.reload(hf)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_dir = \"C:/Users/ctata/Documents/Lab/quality_vectors_git/data/\"\n",
    "fig_dir = \"C:/Users/ctata/Documents/Lab/quality_vectors_git/figures/\"\n",
    "\n",
    "qual_vecs, embed_ids, embed_seqs = hf.getQualVecs(data_dir)\n",
    "\n",
    "\n",
    "\n",
    "otu = pd.read_csv(data_dir + \"halfvarson/seqtab.txt\", sep = \"\\t\")\n",
    "otu.shape\n",
    "\n",
    "mapping = pd.read_csv(data_dir + \"halfvarson/mapping.txt\", sep = \"\\t\")\n",
    "mapping = mapping.set_index(\"sample_name\")\n",
    "\n",
    "\n",
    "best_hits = pd.read_csv(data_dir + \"halfvarson/embed/best_hits.tsv\", header = None, sep = \"\\t\")\n",
    "best_hits.columns = [\"query_id\", \"hit_id\", \"query_seq\", \"hit_seq\", \"evalue\", \"bitscore\"]\n",
    "best_hits = best_hits.set_index('query_id')\n",
    "keep = [i < 1E-29 for i in best_hits['evalue'] ]\n",
    "best_hits = best_hits.loc[keep, :]\n",
    "print(best_hits.shape)\n",
    "best_hits\n",
    "\n",
    "\n",
    "#Get only those ASVs that have a match in the embedding set\n",
    "\n",
    "best_hits = best_hits.loc[[i in otu.columns.values for i in best_hits['query_seq']], :]\n",
    "otu_use = otu.loc[:, best_hits['query_seq'].values]\n",
    "\n",
    "#Assign id of best match \n",
    "otu_use.columns = best_hits.loc[:, 'hit_id']\n",
    "\n",
    "#Put transformation matrix in order to be dotted with the ASV table\n",
    "qual_vecs_half = qual_vecs.loc[otu_use.columns.values, :]\n",
    "\n",
    "\n",
    "#Keep only diagnoses we're interested in right now\n",
    "mapping['sample_name'] = mapping.index.values\n",
    "keep = [i in [\"CD\", \"UC\", \"HC\"] for i in mapping[\"diagnosis_full\"].values]\n",
    "mapping = mapping.loc[keep, :]\n",
    "mapping = mapping.loc[[i in otu_use.index.values for i in mapping.index.values], :]\n",
    "otu_use = otu_use.loc[mapping.index, :]\n",
    "otu_all = otu.loc[mapping.index, :]\n",
    "\n",
    "map_half = mapping\n",
    "otu_half = otu_use\n",
    "print(otu_use.shape)\n",
    "print(otu_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "otu_use.shape\n",
    "mapping.shape\n",
    "len(Counter(mapping.patientnumber))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_half = [i in [\"CD\", \"UC\"] for i in map_half['diagnosis_full']]\n",
    "embedded_half = pd.DataFrame(np.dot(hf.asinh(otu_half), qual_vecs_half))\n",
    "embedded_half.columns = qual_vecs_half.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26726\n",
      "26726\n",
      "(26726, 100)\n",
      "all good\n",
      "all good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load data\n",
    "filt = \".07\"\n",
    "f = open(data_dir + \"AG_new/filter_.07//otu_train_\" + str(filt) + \".obj\", \"rb\")\n",
    "otu_train_ag = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(data_dir + \"AG_new/filter_.07//otu_test_\" + str(filt) + \".obj\", \"rb\")\n",
    "otu_test_ag = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"C:/Users/ctata/Documents/Lab/quality_vectors/data/AG_new/feces/map_train_strictDiag.obj\", \"rb\")\n",
    "map_train_ag = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"C:/Users/ctata/Documents/Lab/quality_vectors/data/AG_new/feces/map_test_strictDiag.obj\", \"rb\")\n",
    "map_test_ag = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "qual_vecs_ag, embed_ids, embed_seqs = hf.getQualVecs(data_dir)\n",
    "\n",
    "otu_train_ag = hf.matchOtuQual(otu_train_ag, qual_vecs_ag.index.values, embed_seqs)\n",
    "otu_test_ag = hf.matchOtuQual(otu_test_ag, qual_vecs_ag.index.values, embed_seqs)\n",
    "\n",
    "data_ag = pd.concat([otu_train_ag, otu_test_ag])\n",
    "mapping_ag = pd.concat([map_train_ag, map_test_ag])\n",
    "embedded_ag = pd.DataFrame(np.dot(hf.asinh(data_ag), qual_vecs_ag))\n",
    "embedded_ag.columns = qual_vecs.columns.values\n",
    "y_ag = mapping_ag[\"IBD\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train_ag = pd.DataFrame(np.dot(hf.asinh(otu_train_ag), qual_vecs_ag))\n",
    "embedded_test_ag = pd.DataFrame(np.dot(hf.asinh(otu_test_ag), qual_vecs_ag))\n",
    "y_train_ag = map_train_ag[\"IBD\"].values\n",
    "y_test_ag = map_test_ag[\"IBD\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.038461538461538464\n",
      "f2 0.024390243902439025\n",
      "Accuracy 0.11347517730496454\n",
      "Precision:  1.0\n",
      "Recall:  0.0196078431372549\n",
      "0 Precision:  0.09747292418772563\n",
      "0 Recall:  1.0\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(hf)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "data_ag_reord = data_ag.loc[:, otu_half.columns.values]\n",
    "f = plt.figure(figsize=(15,5))\n",
    "m, auc, auc_train, fpr, tpr, prec, f1, f2, feat_imp_embed= hf.predictIBD(X_train = hf.asinh(data_ag_reord), y_train = y_ag, X_test = hf.asinh(otu_half), y_test = y_half,\n",
    "                                                                     max_depth = 5, n_estimators = 170,  weight = 20,\n",
    "                                                                     plot= False, plot_pr = False, flipped = False)\n",
    "#f.savefig(\"../figures/curves_ag_half_asin.pdf\")\n",
    "\n",
    "\n",
    "#probs = m.predict_proba(hf.asinh(otu_half))\n",
    "preds = m.predict(hf.asinh(otu_half))\n",
    "#print(preds)\n",
    "y_flipped = y_half\n",
    "print(\"f1\", f1)\n",
    "print(\"f2\" , f2)\n",
    "print(\"Accuracy\", np.sum( preds == y_flipped) / len(y_flipped) )\n",
    "print(\"Precision: \", precision_score(y_flipped, preds))\n",
    "print(\"Recall: \", recall_score(y_flipped, preds))\n",
    "print(\"0 Precision: \", precision_score([1 - i for i in y_half], 1 - preds))\n",
    "print(\"0 Recall: \", recall_score([1 - i for i in y_half], 1 - preds))\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.40873634945397813\n",
      "f2 0.30170428374021185\n",
      "Accuracy 0.3280141843971631\n",
      "Precision:  1.0\n",
      "Recall:  0.2568627450980392\n",
      "0 Precision:  0.12471131639722864\n",
      "0 Recall:  1.0\n",
      "[1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#whole halfvarson dataset\n",
    "importlib.reload(hf)\n",
    "f = plt.figure(figsize=(15,5))\n",
    "m_embed, auc, auc_train, fpr, tpr, prec, f1, f2, feat_imp_embed = hf.predictIBD(X_train = embedded_ag, y_train = y_ag, X_test = embedded_half, y_test = y_half, \n",
    "                                                                            max_depth = 5, n_estimators = 95,  weight = 20,\n",
    "                                                                            plot= False, plot_pr = False, flipped = False)\n",
    "\n",
    "preds = m_embed.predict(embedded_half)\n",
    "y_flipped = y_half\n",
    "\n",
    "print(\"f1\", f1)\n",
    "print(\"f2\" , f2)\n",
    "print(\"Accuracy\", np.sum(preds == y_flipped) / len(y_flipped) )\n",
    "print(\"Precision: \", precision_score(y_flipped, preds))\n",
    "print(\"Recall: \", recall_score(y_flipped, preds))\n",
    "print(\"0 Precision: \", precision_score([1 - i for i in y_flipped], 1 - preds))\n",
    "print(\"0 Recall: \", recall_score([1 - i for i in y_flipped], 1 - preds))\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
